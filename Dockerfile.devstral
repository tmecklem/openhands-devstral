# Use the official vLLM image as base
FROM vllm/vllm-openai:latest

# Set working directory
WORKDIR /app

# Install additional dependencies if needed
RUN pip install --no-cache-dir \
    huggingface-hub \
    transformers

# Create directories for model and templates
RUN mkdir -p /app/models /app/src

# Copy the chat template file
COPY ./devstral_chat_template.jinja /app/src/

# Set environment variables for CUDA optimization
ENV CUDA_VISIBLE_DEVICES=0,1
ENV NCCL_P2P_DISABLE=1
ENV NCCL_IB_DISABLE=1

# Create a startup script
RUN <<EOF cat > /app/start_vllm.sh
#!/bin/bash

# Default values
MODEL_PATH=\${MODEL_PATH:-"/app/models"}
TENSOR_PARALLEL_SIZE=\${TENSOR_PARALLEL_SIZE:-2}
HOST=\${HOST:-"0.0.0.0"}
PORT=\${PORT:-8003}
API_KEY=\${API_KEY:-"token-abc123"}
GPU_MEMORY_UTIL=\${GPU_MEMORY_UTIL:-0.85}
MAX_MODEL_LEN=\${MAX_MODEL_LEN:-16000}
MAX_NUM_SEQS=\${MAX_NUM_SEQS:-1}
CHAT_TEMPLATE=\${CHAT_TEMPLATE:-"/app/src/devstral_chat_template.jinja"}

echo "Starting vLLM with the following configuration:"
echo "Model: \$MODEL_PATH"
echo "Tensor Parallel Size: \$TENSOR_PARALLEL_SIZE"
echo "GPU Memory Utilization: \$GPU_MEMORY_UTIL"
echo "Max Model Length: \$MAX_MODEL_LEN"
echo "Max Sequences: \$MAX_NUM_SEQS"

python -m vllm.entrypoints.openai.api_server \\
    --model "\$MODEL_PATH" \\
    --tensor-parallel-size "\$TENSOR_PARALLEL_SIZE" \\
    --served-model-name Devstral-Small-2505-fp8 \\
    --host "\$HOST" \\
    --port "\$PORT" \\
    --api-key "\$API_KEY" \\
    --enable-prefix-caching \\
    --gpu-memory-utilization="\$GPU_MEMORY_UTIL" \\
    --max-model-len "\$MAX_MODEL_LEN" \\
    --max-num-seqs "\$MAX_NUM_SEQS" \\
    --chat-template "\$CHAT_TEMPLATE" \\
    --tokenizer-mode mistral
EOF

# Make the script executable
RUN chmod +x /app/start_vllm.sh

# Expose the port
EXPOSE 8003

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8003/health || exit 1

# Default command
CMD ["/app/start_vllm.sh"]
