# Use the official vLLM image as base
FROM vllm/vllm-openai:latest

# Set working directory
WORKDIR /app

# Install additional dependencies if needed
RUN pip install --no-cache-dir \
    huggingface-hub \
    transformers

# Create directories for model and templates
RUN mkdir -p /app/models /app/src

# Copy the chat template file
COPY ./devstral_chat_template.jinja /app/src/

# Set environment variables for CUDA optimization
ENV CUDA_VISIBLE_DEVICES=0,1

# Expose the port
EXPOSE 5001

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:5001/health || exit 1

# Use the vLLM entrypoint but provide our custom arguments
CMD ["--model", "/app/models", \
     "--tensor-parallel-size", "2", \
     "--served-model-name", "Devstral-Small-2505-fp8", \
     "--host", "0.0.0.0", \
     "--port", "5001", \
     "--api-key", "token-abc123", \
     "--enable-prefix-caching", \
     "--gpu-memory-utilization", "0.85", \
     "--max-model-len", "16000", \
     "--max-num-seqs", "1", \
     "--chat-template", "/app/src/devstral_chat_template.jinja", \
     "--tokenizer-mode", "mistral", \
     "--enforce-eager", \
     "--quantization", "compressed-tensors"]
